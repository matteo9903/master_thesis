{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d0196",
   "metadata": {},
   "source": [
    "## FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a24ff3",
   "metadata": {},
   "source": [
    "The selection of the best model is made with regards to the cross validation F1 score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINIMUM REDUNDANCY - MAXIMUM RELEVANCE FUNCTION\n",
    "def MRMR(X, y, mod):\n",
    "    from mrmr.pandas import mrmr_classif\n",
    "    ss = []\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
    "    \n",
    "    \n",
    "    vect_feat=np.arange(1,X.shape[1]+1)\n",
    "    for i in vect_feat:\n",
    "        features = mrmr_classif(X=X, y=y, K=i)    \n",
    "        X_mrmr = X.copy()\n",
    "        y_mrmr = y.copy()\n",
    "        X_mrmr = X_mrmr.loc[:,features]\n",
    "        s = cross_val_score(mod, X, y, cv=cv,scoring='f1_weighted').mean()\n",
    "        ss.append(s)\n",
    "        \n",
    "        if(s==max(ss)):\n",
    "            selected_features = list(features)\n",
    "\n",
    "    \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3700698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECURSIVE FEATURE ELIMINATION FUNCTION\n",
    "\n",
    "def recursive_feature_elimination(X_train, y_train, model):\n",
    "    from sklearn.feature_selection import RFECV\n",
    "    from catboost import CatBoostClassifier\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "    rfecv = RFECV(\n",
    "    estimator=model,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=\"f1_weighted\",\n",
    "    min_features_to_select=1,\n",
    "    n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    rfecv.fit(X_train, y_train)\n",
    "    \n",
    "    print('Score: {s}'.format(s=max(rfecv.cv_results_['mean_test_score'])))\n",
    "    print('Number of features: {n}'.format(n=rfecv.n_features_))\n",
    "    \n",
    "    rank = rfecv.ranking_\n",
    "    feats = np.where(rank==1)\n",
    "    selected_features = rfecv.feature_names_in_[feats]\n",
    "    \n",
    "    return list(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ceb142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE for Multilayer Perceptron, with eli5 package for permutation importance\n",
    "\n",
    "def recursive_feature_elimination_MLP(X_train, y_train, model):\n",
    "    import eli5\n",
    "    from eli5.sklearn import PermutationImportance\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    score = []\n",
    "    score.append(cross_val_score(model, X_train, y_train, cv=cv,scoring='f1_weighted').mean())\n",
    "    X_sel = [X_train.copy()]\n",
    "    \n",
    "    feats = []\n",
    "    \n",
    "    for i in range(0,len(X_train.columns)-1):\n",
    "\n",
    "        perm = PermutationImportance(model, cv=cv, scoring='f1_weighted', refit=True)\n",
    "        perm.fit(X_sel[i],y_train)\n",
    "\n",
    "        feat_imp = perm.feature_importances_\n",
    "        idx = np.where(feat_imp == min(feat_imp))    \n",
    "\n",
    "        new_X = X_sel[i].drop(X_sel[i].columns[idx], axis = 1)\n",
    "        X_sel.append(new_X)\n",
    "\n",
    "        score.append(cross_val_score(model, X_sel[i], y_train, cv=cv,scoring='f1_weighted').mean())\n",
    "\n",
    "\n",
    "    X_sel.reverse()   \n",
    "    score.reverse()\n",
    "    \n",
    "    ind = score.index(max(score))\n",
    "    feats = list(X_sel[ind].columns)\n",
    "    \n",
    "    print('Number of features selected: {f}'.format(f=ind+1))\n",
    "    print('Score: {s} '.format(s=max(score)))\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec19138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward/Backward Feature Selection function\n",
    "\n",
    "\n",
    "def fw_bw_feature_selection(X_train,y_train,model , option='forward'):\n",
    "    \n",
    "    from sklearn.feature_selection import SequentialFeatureSelector\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from catboost import CatBoostClassifier\n",
    "    import pandas as pd\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for n in range(len(X_train.columns)):\n",
    "        \n",
    "        if (n<len(X_train.columns)-1):\n",
    "            selector = SequentialFeatureSelector(model, n_features_to_select=n+1, direction=option, scoring='f1_weighted', cv=cv)\n",
    "            X_new = pd.DataFrame(selector.fit_transform(X_train, y_train))\n",
    "            new_score = cross_val_score(model, X_new, y_train, cv=cv, scoring='f1_weighted').mean()\n",
    "        else:\n",
    "            new_score = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1_weighted').mean()\n",
    "        \n",
    "        scores.append(new_score)\n",
    "        \n",
    "        if(new_score==max(scores)):\n",
    "            selected_features = list(selector.get_feature_names_out())\n",
    "    \n",
    "    print('{m} features selected'.format(m=len(selected_features)))\n",
    "    print('Best score: {s}\\n----------------------'.format(s=max(scores)))\n",
    "    \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf71c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26e67540",
   "metadata": {},
   "source": [
    "## MODEL FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(classifier,parameters, X_train, y_train, X_test, y_test, sw_train, sw_test):\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "    from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, roc_auc_score\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "    gs = GridSearchCV(classifier, parameters, cv=cv, scoring = 'f1_weighted', n_jobs=-1, refit=True, return_train_score=True, verbose=10)\n",
    "    X_tr=X_train\n",
    "    X_t=X_test\n",
    "    \n",
    "    \n",
    "    # parameters rto gove to fit method for sample weighting\n",
    "    fit_params = {'sample_weight': sw_train}\n",
    "\n",
    "    gs1=gs.fit(X_tr,y_train,**fit_params)\n",
    "    best_model=gs1.best_estimator_\n",
    "    best_model.fit(X_tr,y_train,**fit_params)\n",
    "\n",
    "    gs_f1 = gs1.best_score_\n",
    "    \n",
    "    # for the evaluation F1 weighted and balanced accuracy are used\n",
    "    cv_f1 = cross_val_score(best_model,X_tr,y_train,scoring='f1_weighted', cv=cv).mean()\n",
    "    cv_acc = cross_val_score(best_model,X_tr,y_train,scoring='balanced_accuracy', cv=cv).mean()\n",
    "    \n",
    "    best_params = gs1.best_params_\n",
    "    \n",
    "    y_pred = best_model.predict(X_t)\n",
    "    y_pred_train = best_model.predict(X_tr)\n",
    "\n",
    "    F1_train = f1_score(y_train, y_pred_train, average=\"weighted\", sample_weight=sw_train)\n",
    "    F1_test = f1_score(y_test, y_pred, average=\"weighted\", sample_weight=sw_test)\n",
    "    acc_train = balanced_accuracy_score(y_train, y_pred_train, sample_weight=sw_train)\n",
    "    acc_test = balanced_accuracy_score(y_test, y_pred, sample_weight=sw_test)\n",
    "\n",
    "    y_probs = best_model.predict_proba(X_t)\n",
    "    AUC = roc_auc_score(y_test, y_probs[:,1])\n",
    "    \n",
    "    scores = [cv_f1, F1_train, F1_test, cv_acc, acc_train, acc_test, AUC, best_model, best_params]\n",
    "    \n",
    "    print('\\n\\n\\n GridSearch Result: ', gs_f1)\n",
    "        \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7995728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as the previous function but without fit params because MLP doesn't support it\n",
    "\n",
    "def fit_model_MLP(classifier,parameters, X_train, y_train, X_test, y_test, sw_train, sw_test):\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "    from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, roc_auc_score\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    gs = GridSearchCV(classifier, parameters, cv=cv, scoring = 'f1_weighted', n_jobs=-1, refit=True, return_train_score=True, verbose=10)\n",
    "    X_tr=X_train\n",
    "    X_t=X_test\n",
    "\n",
    "    gs1=gs.fit(X_tr,y_train)\n",
    "    best_model=gs1.best_estimator_\n",
    "    best_model.fit(X_tr,y_train)\n",
    "    \n",
    "    gs_f1 = gs1.best_score_\n",
    "    \n",
    "    cv_f1 = cross_val_score(best_model,X_tr,y_train,scoring='f1_weighted', cv=cv).mean()\n",
    "    cv_acc = cross_val_score(best_model,X_tr,y_train,scoring='balanced_accuracy', cv=cv).mean()\n",
    "    \n",
    "    best_params = gs1.best_params_\n",
    "    \n",
    "    y_pred = best_model.predict(X_t)\n",
    "    y_pred_train = best_model.predict(X_tr)\n",
    "\n",
    "    F1_train = f1_score(y_train, y_pred_train, average=\"weighted\", sample_weight=sw_train)\n",
    "    F1_test = f1_score(y_test, y_pred, average=\"weighted\", sample_weight=sw_test)\n",
    "    acc_train = balanced_accuracy_score(y_train, y_pred_train, sample_weight=sw_train)\n",
    "    acc_test = balanced_accuracy_score(y_test, y_pred, sample_weight=sw_test)\n",
    "\n",
    "    y_probs = best_model.predict_proba(X_t)\n",
    "    AUC = roc_auc_score(y_test, y_probs[:,1])\n",
    "    \n",
    "    scores = [cv_f1, F1_train, F1_test, cv_acc, acc_train, acc_test, AUC, best_model, best_params]\n",
    "    \n",
    "    print('\\n\\n\\n GridSearch Result: ', gs_f1)\n",
    "        \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb039b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that prints the F1 and accuracy on train and test set, as well as the hyperparamters chosen by gridsearch \n",
    "\n",
    "def print_scores(scores):\n",
    "    print('TRAINING SET\\n')\n",
    "    print('F1: {ftr}'.format(ftr=scores[1]))\n",
    "    print('Accuracy: {atr}\\n---------------------------------\\n'.format(atr=scores[4]))\n",
    "    print('CV SET\\n')\n",
    "    print('F1: {fcv}'.format(fcv=scores[0]))\n",
    "    print('Accuracy: {acv}\\n---------------------------------\\n'.format(acv=scores[3]))\n",
    "    print('TEST SET\\n')\n",
    "    print('F1: {ft}'.format(ft=scores[2]))\n",
    "    print('Accuracy: {at}\\n---------------------------------\\n'.format(at=scores[5]))\n",
    "    print('Parameters: {par}'.format(par=scores[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c7552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that prints and saves the classification report, the confusion matrix and the ROC curve\n",
    "\n",
    "def print_report(best_model, AUC, model_name, X_test, y_test, sw_test, path_params):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import roc_curve, classification_report,confusion_matrix\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    y_pred=best_model.predict(X_test)\n",
    "    y_probs = best_model.predict_proba(X_test)\n",
    "\n",
    "    print()\n",
    "    rep = classification_report(y_test, y_pred, output_dict=True,sample_weight=sw_test)\n",
    "    print(classification_report(y_test, y_pred, sample_weight=sw_test))\n",
    "    rep = pd.DataFrame(rep).transpose()\n",
    "    path='results\\classification\\{o}\\{sel}\\Report_{m}.xlsx'.format(o=path_params[0], sel=path_params[1], m=model_name);\n",
    "    rep.to_excel(path)\n",
    "    \n",
    "    \n",
    "# Plot confusion matrix\n",
    "    sns_plot=sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\")\n",
    "    sns_plot.set_title(\"Confusion Matrix of \"+model_name)\n",
    "    fig = sns_plot.get_figure()\n",
    "    path='results\\classification\\{o}\\{sel}\\Matrix_{m}.png'.format(o=path_params[0], sel=path_params[1], m=model_name);\n",
    "    fig.savefig(path,format=\"png\")\n",
    "    \n",
    "# Plot ROC curve \n",
    "    plt.figure()\n",
    "    fpr, tpr, thresholds=roc_curve(y_test,  y_probs[:,1])\n",
    "    plt.plot(fpr, tpr, label='AUC = %.2f '%AUC)\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve of '+model_name)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    path='results\\classification\\{o}\\{sel}\\ROC_{m}.png'.format(o=path_params[0], sel=path_params[1], m=model_name);\n",
    "    plt.savefig(path,format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd7bd94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79fccac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad6f29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
